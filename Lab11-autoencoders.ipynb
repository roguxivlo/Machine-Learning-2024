{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab11-autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 11 - Autoencoders\n",
        "### Author: Szymon Nowakowski"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Introduction\n",
        "--------------\n",
        "\n",
        "Autoencoders can be thought of as nonlinear extensions of PCA. In this class, we’ll train an autoencoder on the MNIST dataset and compare its encoded representation to the PCA space we constructed earlier (remember our very first class?). This comparison will help us see whether the autoencoder captures the structure of the data more effectively.\n",
        "\n",
        "Next, we’ll put the trained autoencoder to practical use: image denoising.\n",
        "\n",
        "You’ll also notice that throughout this session, we’re treating the images in a class-diagnostic, unsupervised manner—focusing on the structure of the data itself, rather than on labels."
      ],
      "metadata": {
        "id": "fMvw34S08ZvN"
      },
      "id": "fMvw34S08ZvN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading MNIST Dataset\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "jBj9rZpumQ2a"
      },
      "id": "jBj9rZpumQ2a"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot\n",
        "\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=2048,\n",
        "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=1,\n",
        "                                         shuffle=False)"
      ],
      "metadata": {
        "id": "K_b3NgK0mT9C"
      },
      "id": "K_b3NgK0mT9C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Sizes\n",
        "-------------------\n",
        "\n",
        "Recall:\n",
        "- Batched labels are of order one. The first (and only) index is a sample index within a batch.\n",
        "- Image batches have order 4. The first index is a sample index within a batch, but a second index has size 1 and thus it is always 0.\n",
        "  - This index represents a Channel number inserted here by `ToTensor()` transformation, always 0.\n",
        "  - It should be retained because we want to use convolutional layers, which explicitly require this order. For RGB images we have 3 channels, for B&W images we have only one channel.\n"
      ],
      "metadata": {
        "id": "KpN_zrBRmd6D"
      },
      "id": "KpN_zrBRmd6D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder and Decoder Networks\n",
        "-----------------\n",
        "\n",
        "Autoencoder is an Encoder followed by a Decoder.\n",
        "\n",
        "Both components are typically CNN neural networks.\n",
        "\n",
        "We will loosely base their structure on LeNet5 neural network. You can find the definition of LeNet5 [here](https://en.wikipedia.org/wiki/LeNet#/media/File:Comparison_image_neural_networks.svg).\n"
      ],
      "metadata": {
        "id": "YAihBeAxndOm"
      },
      "id": "YAihBeAxndOm"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)      # 1x28x28 -> 6x24x24\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)                        # 6x24x24 -> 6x12x12\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)     # 6x12x12 -> 16x8x8\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)                        # 16x8x8  -> 16x4x4\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=4)   # 16x4x4 ->  120x1x1\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc2 = nn.Linear(in_features=84, out_features=num_classes)\n",
        "\n",
        "        # Optional dropout\n",
        "        self.dropout = nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional feature extraction\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))  # Output: (batch_size, 120, 1, 1)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 120)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "JwvCLWJvp065"
      },
      "id": "JwvCLWJvp065",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "----------------------"
      ],
      "metadata": {
        "id": "NFBpFy6FqLhL"
      },
      "id": "NFBpFy6FqLhL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net = LeNet5().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001.\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(16):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)  #explicitly moving the data to the target device\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the LeNet5 object. Please note,\n",
        "                                            # the nonlinear activation after the last layer is NOT applied\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "                                ####You can experiment - comment this line and check, that the loss DOE"
      ],
      "metadata": {
        "id": "uyEA3Qk3qMj6",
        "outputId": "342e3301-66f3-480a-d092-bed624b9674f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uyEA3Qk3qMj6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cuda\n",
            "epoch: 0 batch: 0 current batch loss: 2.307467222213745\n",
            "epoch: 0 batch: 1 current batch loss: 2.2976996898651123\n",
            "epoch: 0 batch: 2 current batch loss: 2.2881007194519043\n",
            "epoch: 0 batch: 3 current batch loss: 2.28049373626709\n",
            "epoch: 0 batch: 4 current batch loss: 2.2679836750030518\n",
            "epoch: 0 batch: 5 current batch loss: 2.256859302520752\n",
            "epoch: 0 batch: 6 current batch loss: 2.2422878742218018\n",
            "epoch: 0 batch: 7 current batch loss: 2.221681833267212\n",
            "epoch: 0 batch: 8 current batch loss: 2.199702739715576\n",
            "epoch: 0 batch: 9 current batch loss: 2.1724071502685547\n",
            "epoch: 0 batch: 10 current batch loss: 2.1372950077056885\n",
            "epoch: 0 batch: 11 current batch loss: 2.0963642597198486\n",
            "epoch: 0 batch: 12 current batch loss: 2.0645651817321777\n",
            "epoch: 0 batch: 13 current batch loss: 2.0240578651428223\n",
            "epoch: 0 batch: 14 current batch loss: 1.9557939767837524\n",
            "epoch: 0 batch: 15 current batch loss: 1.8908413648605347\n",
            "epoch: 0 batch: 16 current batch loss: 1.821674108505249\n",
            "epoch: 0 batch: 17 current batch loss: 1.7525076866149902\n",
            "epoch: 0 batch: 18 current batch loss: 1.6704078912734985\n",
            "epoch: 0 batch: 19 current batch loss: 1.5764952898025513\n",
            "epoch: 0 batch: 20 current batch loss: 1.4858226776123047\n",
            "epoch: 0 batch: 21 current batch loss: 1.3937017917633057\n",
            "epoch: 0 batch: 22 current batch loss: 1.3027068376541138\n",
            "epoch: 0 batch: 23 current batch loss: 1.2155948877334595\n",
            "epoch: 0 batch: 24 current batch loss: 1.13252592086792\n",
            "epoch: 0 batch: 25 current batch loss: 1.0082550048828125\n",
            "epoch: 0 batch: 26 current batch loss: 0.9776633977890015\n",
            "epoch: 0 batch: 27 current batch loss: 0.8936355710029602\n",
            "epoch: 0 batch: 28 current batch loss: 0.8205739855766296\n",
            "epoch: 0 batch: 29 current batch loss: 0.7494543194770813\n",
            "epoch: 1 batch: 0 current batch loss: 0.7445733547210693\n",
            "epoch: 1 batch: 1 current batch loss: 0.676678478717804\n",
            "epoch: 1 batch: 2 current batch loss: 0.6860930919647217\n",
            "epoch: 1 batch: 3 current batch loss: 0.6466779112815857\n",
            "epoch: 1 batch: 4 current batch loss: 0.6835846900939941\n",
            "epoch: 1 batch: 5 current batch loss: 0.6095463037490845\n",
            "epoch: 1 batch: 6 current batch loss: 0.5563364028930664\n",
            "epoch: 1 batch: 7 current batch loss: 0.6416265964508057\n",
            "epoch: 1 batch: 8 current batch loss: 0.5869109630584717\n",
            "epoch: 1 batch: 9 current batch loss: 0.621166467666626\n",
            "epoch: 1 batch: 10 current batch loss: 0.6060394644737244\n",
            "epoch: 1 batch: 11 current batch loss: 0.6267679929733276\n",
            "epoch: 1 batch: 12 current batch loss: 0.5638657212257385\n",
            "epoch: 1 batch: 13 current batch loss: 0.5475528240203857\n",
            "epoch: 1 batch: 14 current batch loss: 0.5912604928016663\n",
            "epoch: 1 batch: 15 current batch loss: 0.5777013897895813\n",
            "epoch: 1 batch: 16 current batch loss: 0.5574648380279541\n",
            "epoch: 1 batch: 17 current batch loss: 0.5204067230224609\n",
            "epoch: 1 batch: 18 current batch loss: 0.5862330198287964\n",
            "epoch: 1 batch: 19 current batch loss: 0.5044187307357788\n",
            "epoch: 1 batch: 20 current batch loss: 0.5595541000366211\n",
            "epoch: 1 batch: 21 current batch loss: 0.5344175696372986\n",
            "epoch: 1 batch: 22 current batch loss: 0.47518694400787354\n",
            "epoch: 1 batch: 23 current batch loss: 0.48938485980033875\n",
            "epoch: 1 batch: 24 current batch loss: 0.49799054861068726\n",
            "epoch: 1 batch: 25 current batch loss: 0.4815172553062439\n",
            "epoch: 1 batch: 26 current batch loss: 0.5118647217750549\n",
            "epoch: 1 batch: 27 current batch loss: 0.4677782654762268\n",
            "epoch: 1 batch: 28 current batch loss: 0.5086766481399536\n",
            "epoch: 1 batch: 29 current batch loss: 0.4663173258304596\n",
            "epoch: 2 batch: 0 current batch loss: 0.5122620463371277\n",
            "epoch: 2 batch: 1 current batch loss: 0.43100032210350037\n",
            "epoch: 2 batch: 2 current batch loss: 0.47366929054260254\n",
            "epoch: 2 batch: 3 current batch loss: 0.4902701675891876\n",
            "epoch: 2 batch: 4 current batch loss: 0.4368821978569031\n",
            "epoch: 2 batch: 5 current batch loss: 0.47879302501678467\n",
            "epoch: 2 batch: 6 current batch loss: 0.46290066838264465\n",
            "epoch: 2 batch: 7 current batch loss: 0.461933434009552\n",
            "epoch: 2 batch: 8 current batch loss: 0.4879026710987091\n",
            "epoch: 2 batch: 9 current batch loss: 0.4121819734573364\n",
            "epoch: 2 batch: 10 current batch loss: 0.44881999492645264\n",
            "epoch: 2 batch: 11 current batch loss: 0.4635748267173767\n",
            "epoch: 2 batch: 12 current batch loss: 0.43383365869522095\n",
            "epoch: 2 batch: 13 current batch loss: 0.41614246368408203\n",
            "epoch: 2 batch: 14 current batch loss: 0.44752493500709534\n",
            "epoch: 2 batch: 15 current batch loss: 0.40318065881729126\n",
            "epoch: 2 batch: 16 current batch loss: 0.42876070737838745\n",
            "epoch: 2 batch: 17 current batch loss: 0.3771544098854065\n",
            "epoch: 2 batch: 18 current batch loss: 0.3956523835659027\n",
            "epoch: 2 batch: 19 current batch loss: 0.39386171102523804\n",
            "epoch: 2 batch: 20 current batch loss: 0.4428822994232178\n",
            "epoch: 2 batch: 21 current batch loss: 0.3976741433143616\n",
            "epoch: 2 batch: 22 current batch loss: 0.36270323395729065\n",
            "epoch: 2 batch: 23 current batch loss: 0.3837597370147705\n",
            "epoch: 2 batch: 24 current batch loss: 0.3874681890010834\n",
            "epoch: 2 batch: 25 current batch loss: 0.4124963581562042\n",
            "epoch: 2 batch: 26 current batch loss: 0.4002905488014221\n",
            "epoch: 2 batch: 27 current batch loss: 0.3900708258152008\n",
            "epoch: 2 batch: 28 current batch loss: 0.4186092019081116\n",
            "epoch: 2 batch: 29 current batch loss: 0.3757488429546356\n",
            "epoch: 3 batch: 0 current batch loss: 0.38971632719039917\n",
            "epoch: 3 batch: 1 current batch loss: 0.3884454667568207\n",
            "epoch: 3 batch: 2 current batch loss: 0.36086657643318176\n",
            "epoch: 3 batch: 3 current batch loss: 0.3786153793334961\n",
            "epoch: 3 batch: 4 current batch loss: 0.3954916298389435\n",
            "epoch: 3 batch: 5 current batch loss: 0.4145154058933258\n",
            "epoch: 3 batch: 6 current batch loss: 0.3627403974533081\n",
            "epoch: 3 batch: 7 current batch loss: 0.3686010539531708\n",
            "epoch: 3 batch: 8 current batch loss: 0.36689433455467224\n",
            "epoch: 3 batch: 9 current batch loss: 0.3693225085735321\n",
            "epoch: 3 batch: 10 current batch loss: 0.3625296652317047\n",
            "epoch: 3 batch: 11 current batch loss: 0.3304405212402344\n",
            "epoch: 3 batch: 12 current batch loss: 0.37868764996528625\n",
            "epoch: 3 batch: 13 current batch loss: 0.3537886142730713\n",
            "epoch: 3 batch: 14 current batch loss: 0.40011751651763916\n",
            "epoch: 3 batch: 15 current batch loss: 0.3657699525356293\n",
            "epoch: 3 batch: 16 current batch loss: 0.34941086173057556\n",
            "epoch: 3 batch: 17 current batch loss: 0.34064286947250366\n",
            "epoch: 3 batch: 18 current batch loss: 0.3284137547016144\n",
            "epoch: 3 batch: 19 current batch loss: 0.3664250373840332\n",
            "epoch: 3 batch: 20 current batch loss: 0.3382791578769684\n",
            "epoch: 3 batch: 21 current batch loss: 0.38589903712272644\n",
            "epoch: 3 batch: 22 current batch loss: 0.321664035320282\n",
            "epoch: 3 batch: 23 current batch loss: 0.34843188524246216\n",
            "epoch: 3 batch: 24 current batch loss: 0.31724783778190613\n",
            "epoch: 3 batch: 25 current batch loss: 0.3264232277870178\n",
            "epoch: 3 batch: 26 current batch loss: 0.34318041801452637\n",
            "epoch: 3 batch: 27 current batch loss: 0.3332633376121521\n",
            "epoch: 3 batch: 28 current batch loss: 0.33802303671836853\n",
            "epoch: 3 batch: 29 current batch loss: 0.3525042235851288\n",
            "epoch: 4 batch: 0 current batch loss: 0.3251681625843048\n",
            "epoch: 4 batch: 1 current batch loss: 0.3460403084754944\n",
            "epoch: 4 batch: 2 current batch loss: 0.36438488960266113\n",
            "epoch: 4 batch: 3 current batch loss: 0.3118062913417816\n",
            "epoch: 4 batch: 4 current batch loss: 0.3124235272407532\n",
            "epoch: 4 batch: 5 current batch loss: 0.310992956161499\n",
            "epoch: 4 batch: 6 current batch loss: 0.3333747088909149\n",
            "epoch: 4 batch: 7 current batch loss: 0.31037238240242004\n",
            "epoch: 4 batch: 8 current batch loss: 0.32411742210388184\n",
            "epoch: 4 batch: 9 current batch loss: 0.3126837909221649\n",
            "epoch: 4 batch: 10 current batch loss: 0.30423790216445923\n",
            "epoch: 4 batch: 11 current batch loss: 0.3391048014163971\n",
            "epoch: 4 batch: 12 current batch loss: 0.2807716727256775\n",
            "epoch: 4 batch: 13 current batch loss: 0.3004794418811798\n",
            "epoch: 4 batch: 14 current batch loss: 0.3260076940059662\n",
            "epoch: 4 batch: 15 current batch loss: 0.31779882311820984\n",
            "epoch: 4 batch: 16 current batch loss: 0.29296448826789856\n",
            "epoch: 4 batch: 17 current batch loss: 0.2817234396934509\n",
            "epoch: 4 batch: 18 current batch loss: 0.2710074186325073\n",
            "epoch: 4 batch: 19 current batch loss: 0.28351128101348877\n",
            "epoch: 4 batch: 20 current batch loss: 0.2909514605998993\n",
            "epoch: 4 batch: 21 current batch loss: 0.27334821224212646\n",
            "epoch: 4 batch: 22 current batch loss: 0.29864344000816345\n",
            "epoch: 4 batch: 23 current batch loss: 0.29591041803359985\n",
            "epoch: 4 batch: 24 current batch loss: 0.31514716148376465\n",
            "epoch: 4 batch: 25 current batch loss: 0.2776168882846832\n",
            "epoch: 4 batch: 26 current batch loss: 0.31777071952819824\n",
            "epoch: 4 batch: 27 current batch loss: 0.24972854554653168\n",
            "epoch: 4 batch: 28 current batch loss: 0.27913063764572144\n",
            "epoch: 4 batch: 29 current batch loss: 0.2980455160140991\n",
            "epoch: 5 batch: 0 current batch loss: 0.2687731981277466\n",
            "epoch: 5 batch: 1 current batch loss: 0.28064948320388794\n",
            "epoch: 5 batch: 2 current batch loss: 0.2911888659000397\n",
            "epoch: 5 batch: 3 current batch loss: 0.27363091707229614\n",
            "epoch: 5 batch: 4 current batch loss: 0.28360259532928467\n",
            "epoch: 5 batch: 5 current batch loss: 0.2567523717880249\n",
            "epoch: 5 batch: 6 current batch loss: 0.29010888934135437\n",
            "epoch: 5 batch: 7 current batch loss: 0.2810553014278412\n",
            "epoch: 5 batch: 8 current batch loss: 0.22633026540279388\n",
            "epoch: 5 batch: 9 current batch loss: 0.2506321668624878\n",
            "epoch: 5 batch: 10 current batch loss: 0.24272345006465912\n",
            "epoch: 5 batch: 11 current batch loss: 0.2706359028816223\n",
            "epoch: 5 batch: 12 current batch loss: 0.2868557572364807\n",
            "epoch: 5 batch: 13 current batch loss: 0.2573353946208954\n",
            "epoch: 5 batch: 14 current batch loss: 0.29555192589759827\n",
            "epoch: 5 batch: 15 current batch loss: 0.2578599750995636\n",
            "epoch: 5 batch: 16 current batch loss: 0.24423715472221375\n",
            "epoch: 5 batch: 17 current batch loss: 0.2200559377670288\n",
            "epoch: 5 batch: 18 current batch loss: 0.24587947130203247\n",
            "epoch: 5 batch: 19 current batch loss: 0.24412912130355835\n",
            "epoch: 5 batch: 20 current batch loss: 0.25127947330474854\n",
            "epoch: 5 batch: 21 current batch loss: 0.2507615089416504\n",
            "epoch: 5 batch: 22 current batch loss: 0.2488987147808075\n",
            "epoch: 5 batch: 23 current batch loss: 0.2591194808483124\n",
            "epoch: 5 batch: 24 current batch loss: 0.24642173945903778\n",
            "epoch: 5 batch: 25 current batch loss: 0.23942506313323975\n",
            "epoch: 5 batch: 26 current batch loss: 0.2311294823884964\n",
            "epoch: 5 batch: 27 current batch loss: 0.24128223955631256\n",
            "epoch: 5 batch: 28 current batch loss: 0.20241506397724152\n",
            "epoch: 5 batch: 29 current batch loss: 0.2765153646469116\n",
            "epoch: 6 batch: 0 current batch loss: 0.22414471209049225\n",
            "epoch: 6 batch: 1 current batch loss: 0.23766517639160156\n",
            "epoch: 6 batch: 2 current batch loss: 0.20841187238693237\n",
            "epoch: 6 batch: 3 current batch loss: 0.23391608893871307\n",
            "epoch: 6 batch: 4 current batch loss: 0.22185903787612915\n",
            "epoch: 6 batch: 5 current batch loss: 0.22026744484901428\n",
            "epoch: 6 batch: 6 current batch loss: 0.2353641539812088\n",
            "epoch: 6 batch: 7 current batch loss: 0.2012433558702469\n",
            "epoch: 6 batch: 8 current batch loss: 0.22590160369873047\n",
            "epoch: 6 batch: 9 current batch loss: 0.25597792863845825\n",
            "epoch: 6 batch: 10 current batch loss: 0.18537786602973938\n",
            "epoch: 6 batch: 11 current batch loss: 0.23315392434597015\n",
            "epoch: 6 batch: 12 current batch loss: 0.22159822285175323\n",
            "epoch: 6 batch: 13 current batch loss: 0.2517637014389038\n",
            "epoch: 6 batch: 14 current batch loss: 0.2329796850681305\n",
            "epoch: 6 batch: 15 current batch loss: 0.21714317798614502\n",
            "epoch: 6 batch: 16 current batch loss: 0.2099473923444748\n",
            "epoch: 6 batch: 17 current batch loss: 0.20414292812347412\n",
            "epoch: 6 batch: 18 current batch loss: 0.19017626345157623\n",
            "epoch: 6 batch: 19 current batch loss: 0.20842179656028748\n",
            "epoch: 6 batch: 20 current batch loss: 0.2062530815601349\n",
            "epoch: 6 batch: 21 current batch loss: 0.18926337361335754\n",
            "epoch: 6 batch: 22 current batch loss: 0.2170426994562149\n",
            "epoch: 6 batch: 23 current batch loss: 0.22682827711105347\n",
            "epoch: 6 batch: 24 current batch loss: 0.20818834006786346\n",
            "epoch: 6 batch: 25 current batch loss: 0.2134050726890564\n",
            "epoch: 6 batch: 26 current batch loss: 0.17400053143501282\n",
            "epoch: 6 batch: 27 current batch loss: 0.19583022594451904\n",
            "epoch: 6 batch: 28 current batch loss: 0.21238450706005096\n",
            "epoch: 6 batch: 29 current batch loss: 0.16402840614318848\n",
            "epoch: 7 batch: 0 current batch loss: 0.19663909077644348\n",
            "epoch: 7 batch: 1 current batch loss: 0.19551953673362732\n",
            "epoch: 7 batch: 2 current batch loss: 0.16042819619178772\n",
            "epoch: 7 batch: 3 current batch loss: 0.2056373655796051\n",
            "epoch: 7 batch: 4 current batch loss: 0.21016688644886017\n",
            "epoch: 7 batch: 5 current batch loss: 0.2097111940383911\n",
            "epoch: 7 batch: 6 current batch loss: 0.20098643004894257\n",
            "epoch: 7 batch: 7 current batch loss: 0.2006053626537323\n",
            "epoch: 7 batch: 8 current batch loss: 0.16089579463005066\n",
            "epoch: 7 batch: 9 current batch loss: 0.18365313112735748\n",
            "epoch: 7 batch: 10 current batch loss: 0.17605602741241455\n",
            "epoch: 7 batch: 11 current batch loss: 0.17040061950683594\n",
            "epoch: 7 batch: 12 current batch loss: 0.1914072483778\n",
            "epoch: 7 batch: 13 current batch loss: 0.18862880766391754\n",
            "epoch: 7 batch: 14 current batch loss: 0.16649186611175537\n",
            "epoch: 7 batch: 15 current batch loss: 0.16985425353050232\n",
            "epoch: 7 batch: 16 current batch loss: 0.171727254986763\n",
            "epoch: 7 batch: 17 current batch loss: 0.18273654580116272\n",
            "epoch: 7 batch: 18 current batch loss: 0.17020206153392792\n",
            "epoch: 7 batch: 19 current batch loss: 0.17205099761486053\n",
            "epoch: 7 batch: 20 current batch loss: 0.18174035847187042\n",
            "epoch: 7 batch: 21 current batch loss: 0.18014299869537354\n",
            "epoch: 7 batch: 22 current batch loss: 0.16985011100769043\n",
            "epoch: 7 batch: 23 current batch loss: 0.1719866394996643\n",
            "epoch: 7 batch: 24 current batch loss: 0.1947534680366516\n",
            "epoch: 7 batch: 25 current batch loss: 0.18841925263404846\n",
            "epoch: 7 batch: 26 current batch loss: 0.15697844326496124\n",
            "epoch: 7 batch: 27 current batch loss: 0.16088102757930756\n",
            "epoch: 7 batch: 28 current batch loss: 0.15679076313972473\n",
            "epoch: 7 batch: 29 current batch loss: 0.16770757734775543\n",
            "epoch: 8 batch: 0 current batch loss: 0.15249615907669067\n",
            "epoch: 8 batch: 1 current batch loss: 0.14959919452667236\n",
            "epoch: 8 batch: 2 current batch loss: 0.14970919489860535\n",
            "epoch: 8 batch: 3 current batch loss: 0.16072434186935425\n",
            "epoch: 8 batch: 4 current batch loss: 0.13479071855545044\n",
            "epoch: 8 batch: 5 current batch loss: 0.14296351373195648\n",
            "epoch: 8 batch: 6 current batch loss: 0.15271008014678955\n",
            "epoch: 8 batch: 7 current batch loss: 0.18350644409656525\n",
            "epoch: 8 batch: 8 current batch loss: 0.16977573931217194\n",
            "epoch: 8 batch: 9 current batch loss: 0.1761929839849472\n",
            "epoch: 8 batch: 10 current batch loss: 0.14931149780750275\n",
            "epoch: 8 batch: 11 current batch loss: 0.13999204337596893\n",
            "epoch: 8 batch: 12 current batch loss: 0.166209414601326\n",
            "epoch: 8 batch: 13 current batch loss: 0.14958468079566956\n",
            "epoch: 8 batch: 14 current batch loss: 0.17557966709136963\n",
            "epoch: 8 batch: 15 current batch loss: 0.1654737889766693\n",
            "epoch: 8 batch: 16 current batch loss: 0.12909819185733795\n",
            "epoch: 8 batch: 17 current batch loss: 0.13036088645458221\n",
            "epoch: 8 batch: 18 current batch loss: 0.14031659066677094\n",
            "epoch: 8 batch: 19 current batch loss: 0.1536332070827484\n",
            "epoch: 8 batch: 20 current batch loss: 0.14519381523132324\n",
            "epoch: 8 batch: 21 current batch loss: 0.16468054056167603\n",
            "epoch: 8 batch: 22 current batch loss: 0.14673973619937897\n",
            "epoch: 8 batch: 23 current batch loss: 0.14506269991397858\n",
            "epoch: 8 batch: 24 current batch loss: 0.15181982517242432\n",
            "epoch: 8 batch: 25 current batch loss: 0.15306220948696136\n",
            "epoch: 8 batch: 26 current batch loss: 0.12712602317333221\n",
            "epoch: 8 batch: 27 current batch loss: 0.14098890125751495\n",
            "epoch: 8 batch: 28 current batch loss: 0.1548406332731247\n",
            "epoch: 8 batch: 29 current batch loss: 0.14394645392894745\n",
            "epoch: 9 batch: 0 current batch loss: 0.13636526465415955\n",
            "epoch: 9 batch: 1 current batch loss: 0.12743450701236725\n",
            "epoch: 9 batch: 2 current batch loss: 0.16403689980506897\n",
            "epoch: 9 batch: 3 current batch loss: 0.12408460676670074\n",
            "epoch: 9 batch: 4 current batch loss: 0.1481965184211731\n",
            "epoch: 9 batch: 5 current batch loss: 0.1353626400232315\n",
            "epoch: 9 batch: 6 current batch loss: 0.1345560997724533\n",
            "epoch: 9 batch: 7 current batch loss: 0.14996390044689178\n",
            "epoch: 9 batch: 8 current batch loss: 0.14844150841236115\n",
            "epoch: 9 batch: 9 current batch loss: 0.11403413861989975\n",
            "epoch: 9 batch: 10 current batch loss: 0.1365317553281784\n",
            "epoch: 9 batch: 11 current batch loss: 0.11397843807935715\n",
            "epoch: 9 batch: 12 current batch loss: 0.1418653279542923\n",
            "epoch: 9 batch: 13 current batch loss: 0.12269283086061478\n",
            "epoch: 9 batch: 14 current batch loss: 0.11113439500331879\n",
            "epoch: 9 batch: 15 current batch loss: 0.12293317914009094\n",
            "epoch: 9 batch: 16 current batch loss: 0.13287419080734253\n",
            "epoch: 9 batch: 17 current batch loss: 0.1202285885810852\n",
            "epoch: 9 batch: 18 current batch loss: 0.102260060608387\n",
            "epoch: 9 batch: 19 current batch loss: 0.13931217789649963\n",
            "epoch: 9 batch: 20 current batch loss: 0.1291271150112152\n",
            "epoch: 9 batch: 21 current batch loss: 0.11187846958637238\n",
            "epoch: 9 batch: 22 current batch loss: 0.12057168036699295\n",
            "epoch: 9 batch: 23 current batch loss: 0.1352117955684662\n",
            "epoch: 9 batch: 24 current batch loss: 0.1396797150373459\n",
            "epoch: 9 batch: 25 current batch loss: 0.13492554426193237\n",
            "epoch: 9 batch: 26 current batch loss: 0.13495542109012604\n",
            "epoch: 9 batch: 27 current batch loss: 0.131479412317276\n",
            "epoch: 9 batch: 28 current batch loss: 0.12344512343406677\n",
            "epoch: 9 batch: 29 current batch loss: 0.15187469124794006\n",
            "epoch: 10 batch: 0 current batch loss: 0.11881788820028305\n",
            "epoch: 10 batch: 1 current batch loss: 0.10555583983659744\n",
            "epoch: 10 batch: 2 current batch loss: 0.12593479454517365\n",
            "epoch: 10 batch: 3 current batch loss: 0.13580428063869476\n",
            "epoch: 10 batch: 4 current batch loss: 0.12016358971595764\n",
            "epoch: 10 batch: 5 current batch loss: 0.10521066188812256\n",
            "epoch: 10 batch: 6 current batch loss: 0.11936485022306442\n",
            "epoch: 10 batch: 7 current batch loss: 0.12695440649986267\n",
            "epoch: 10 batch: 8 current batch loss: 0.14469598233699799\n",
            "epoch: 10 batch: 9 current batch loss: 0.1300065666437149\n",
            "epoch: 10 batch: 10 current batch loss: 0.12327652424573898\n",
            "epoch: 10 batch: 11 current batch loss: 0.11139128357172012\n",
            "epoch: 10 batch: 12 current batch loss: 0.11810015141963959\n",
            "epoch: 10 batch: 13 current batch loss: 0.09998931735754013\n",
            "epoch: 10 batch: 14 current batch loss: 0.10643701255321503\n",
            "epoch: 10 batch: 15 current batch loss: 0.13413408398628235\n",
            "epoch: 10 batch: 16 current batch loss: 0.11431199312210083\n",
            "epoch: 10 batch: 17 current batch loss: 0.10529880970716476\n",
            "epoch: 10 batch: 18 current batch loss: 0.12039768695831299\n",
            "epoch: 10 batch: 19 current batch loss: 0.10553140938282013\n",
            "epoch: 10 batch: 20 current batch loss: 0.12336850166320801\n",
            "epoch: 10 batch: 21 current batch loss: 0.1013345867395401\n",
            "epoch: 10 batch: 22 current batch loss: 0.11435195058584213\n",
            "epoch: 10 batch: 23 current batch loss: 0.09959695488214493\n",
            "epoch: 10 batch: 24 current batch loss: 0.09164388477802277\n",
            "epoch: 10 batch: 25 current batch loss: 0.10368216782808304\n",
            "epoch: 10 batch: 26 current batch loss: 0.09739790856838226\n",
            "epoch: 10 batch: 27 current batch loss: 0.14572609961032867\n",
            "epoch: 10 batch: 28 current batch loss: 0.12264253944158554\n",
            "epoch: 10 batch: 29 current batch loss: 0.12335056811571121\n",
            "epoch: 11 batch: 0 current batch loss: 0.09692863374948502\n",
            "epoch: 11 batch: 1 current batch loss: 0.1050332635641098\n",
            "epoch: 11 batch: 2 current batch loss: 0.09930122643709183\n",
            "epoch: 11 batch: 3 current batch loss: 0.09933234006166458\n",
            "epoch: 11 batch: 4 current batch loss: 0.11614810675382614\n",
            "epoch: 11 batch: 5 current batch loss: 0.08957943320274353\n",
            "epoch: 11 batch: 6 current batch loss: 0.10655808448791504\n",
            "epoch: 11 batch: 7 current batch loss: 0.1137431412935257\n",
            "epoch: 11 batch: 8 current batch loss: 0.09338998049497604\n",
            "epoch: 11 batch: 9 current batch loss: 0.1296868920326233\n",
            "epoch: 11 batch: 10 current batch loss: 0.10928388684988022\n",
            "epoch: 11 batch: 11 current batch loss: 0.12064917385578156\n",
            "epoch: 11 batch: 12 current batch loss: 0.09634572267532349\n",
            "epoch: 11 batch: 13 current batch loss: 0.10972145199775696\n",
            "epoch: 11 batch: 14 current batch loss: 0.12097074836492538\n",
            "epoch: 11 batch: 15 current batch loss: 0.10652618855237961\n",
            "epoch: 11 batch: 16 current batch loss: 0.09775864332914352\n",
            "epoch: 11 batch: 17 current batch loss: 0.08757474273443222\n",
            "epoch: 11 batch: 18 current batch loss: 0.10266174376010895\n",
            "epoch: 11 batch: 19 current batch loss: 0.08814813941717148\n",
            "epoch: 11 batch: 20 current batch loss: 0.10092373192310333\n",
            "epoch: 11 batch: 21 current batch loss: 0.0885312631726265\n",
            "epoch: 11 batch: 22 current batch loss: 0.12029183655977249\n",
            "epoch: 11 batch: 23 current batch loss: 0.10314741730690002\n",
            "epoch: 11 batch: 24 current batch loss: 0.10778062790632248\n",
            "epoch: 11 batch: 25 current batch loss: 0.11717583239078522\n",
            "epoch: 11 batch: 26 current batch loss: 0.1033962070941925\n",
            "epoch: 11 batch: 27 current batch loss: 0.1181168407201767\n",
            "epoch: 11 batch: 28 current batch loss: 0.11185542494058609\n",
            "epoch: 11 batch: 29 current batch loss: 0.09263692051172256\n",
            "epoch: 12 batch: 0 current batch loss: 0.07453913241624832\n",
            "epoch: 12 batch: 1 current batch loss: 0.09661273658275604\n",
            "epoch: 12 batch: 2 current batch loss: 0.10896792262792587\n",
            "epoch: 12 batch: 3 current batch loss: 0.11602351069450378\n",
            "epoch: 12 batch: 4 current batch loss: 0.09031114727258682\n",
            "epoch: 12 batch: 5 current batch loss: 0.12116965651512146\n",
            "epoch: 12 batch: 6 current batch loss: 0.09655357152223587\n",
            "epoch: 12 batch: 7 current batch loss: 0.08472289144992828\n",
            "epoch: 12 batch: 8 current batch loss: 0.13531345129013062\n",
            "epoch: 12 batch: 9 current batch loss: 0.09028590470552444\n",
            "epoch: 12 batch: 10 current batch loss: 0.10851580649614334\n",
            "epoch: 12 batch: 11 current batch loss: 0.10504844784736633\n",
            "epoch: 12 batch: 12 current batch loss: 0.10339663177728653\n",
            "epoch: 12 batch: 13 current batch loss: 0.10405852645635605\n",
            "epoch: 12 batch: 14 current batch loss: 0.0990123376250267\n",
            "epoch: 12 batch: 15 current batch loss: 0.0888487845659256\n",
            "epoch: 12 batch: 16 current batch loss: 0.07488704472780228\n",
            "epoch: 12 batch: 17 current batch loss: 0.1045098528265953\n",
            "epoch: 12 batch: 18 current batch loss: 0.11205141246318817\n",
            "epoch: 12 batch: 19 current batch loss: 0.08514220267534256\n",
            "epoch: 12 batch: 20 current batch loss: 0.0898713618516922\n",
            "epoch: 12 batch: 21 current batch loss: 0.09311862289905548\n",
            "epoch: 12 batch: 22 current batch loss: 0.08480843901634216\n",
            "epoch: 12 batch: 23 current batch loss: 0.08320845663547516\n",
            "epoch: 12 batch: 24 current batch loss: 0.10434938222169876\n",
            "epoch: 12 batch: 25 current batch loss: 0.10119152069091797\n",
            "epoch: 12 batch: 26 current batch loss: 0.11255824565887451\n",
            "epoch: 12 batch: 27 current batch loss: 0.09728198498487473\n",
            "epoch: 12 batch: 28 current batch loss: 0.09123199433088303\n",
            "epoch: 12 batch: 29 current batch loss: 0.06954174488782883\n",
            "epoch: 13 batch: 0 current batch loss: 0.07086475193500519\n",
            "epoch: 13 batch: 1 current batch loss: 0.0728989988565445\n",
            "epoch: 13 batch: 2 current batch loss: 0.08341154456138611\n",
            "epoch: 13 batch: 3 current batch loss: 0.0842834934592247\n",
            "epoch: 13 batch: 4 current batch loss: 0.08172979205846786\n",
            "epoch: 13 batch: 5 current batch loss: 0.08711537718772888\n",
            "epoch: 13 batch: 6 current batch loss: 0.08456684648990631\n",
            "epoch: 13 batch: 7 current batch loss: 0.08879417926073074\n",
            "epoch: 13 batch: 8 current batch loss: 0.07697617262601852\n",
            "epoch: 13 batch: 9 current batch loss: 0.1002073809504509\n",
            "epoch: 13 batch: 10 current batch loss: 0.09253378957509995\n",
            "epoch: 13 batch: 11 current batch loss: 0.09313817322254181\n",
            "epoch: 13 batch: 12 current batch loss: 0.08302765339612961\n",
            "epoch: 13 batch: 13 current batch loss: 0.09823119640350342\n",
            "epoch: 13 batch: 14 current batch loss: 0.09881725162267685\n",
            "epoch: 13 batch: 15 current batch loss: 0.07997873425483704\n",
            "epoch: 13 batch: 16 current batch loss: 0.09339173138141632\n",
            "epoch: 13 batch: 17 current batch loss: 0.07867071032524109\n",
            "epoch: 13 batch: 18 current batch loss: 0.08494099974632263\n",
            "epoch: 13 batch: 19 current batch loss: 0.09552337229251862\n",
            "epoch: 13 batch: 20 current batch loss: 0.09433747082948685\n",
            "epoch: 13 batch: 21 current batch loss: 0.0925382599234581\n",
            "epoch: 13 batch: 22 current batch loss: 0.08633960038423538\n",
            "epoch: 13 batch: 23 current batch loss: 0.10270248353481293\n",
            "epoch: 13 batch: 24 current batch loss: 0.08826571702957153\n",
            "epoch: 13 batch: 25 current batch loss: 0.1125110313296318\n",
            "epoch: 13 batch: 26 current batch loss: 0.08113428205251694\n",
            "epoch: 13 batch: 27 current batch loss: 0.09157440811395645\n",
            "epoch: 13 batch: 28 current batch loss: 0.0958591178059578\n",
            "epoch: 13 batch: 29 current batch loss: 0.11784060299396515\n",
            "epoch: 14 batch: 0 current batch loss: 0.08196878433227539\n",
            "epoch: 14 batch: 1 current batch loss: 0.08082710951566696\n",
            "epoch: 14 batch: 2 current batch loss: 0.101069375872612\n",
            "epoch: 14 batch: 3 current batch loss: 0.09415104240179062\n",
            "epoch: 14 batch: 4 current batch loss: 0.0773509219288826\n",
            "epoch: 14 batch: 5 current batch loss: 0.07619848102331161\n",
            "epoch: 14 batch: 6 current batch loss: 0.06462005525827408\n",
            "epoch: 14 batch: 7 current batch loss: 0.07755542546510696\n",
            "epoch: 14 batch: 8 current batch loss: 0.08787832409143448\n",
            "epoch: 14 batch: 9 current batch loss: 0.10409212112426758\n",
            "epoch: 14 batch: 10 current batch loss: 0.08237437158823013\n",
            "epoch: 14 batch: 11 current batch loss: 0.0763598382472992\n",
            "epoch: 14 batch: 12 current batch loss: 0.08479757606983185\n",
            "epoch: 14 batch: 13 current batch loss: 0.07851703464984894\n",
            "epoch: 14 batch: 14 current batch loss: 0.09407977759838104\n",
            "epoch: 14 batch: 15 current batch loss: 0.088993601500988\n",
            "epoch: 14 batch: 16 current batch loss: 0.09062231332063675\n",
            "epoch: 14 batch: 17 current batch loss: 0.07319523394107819\n",
            "epoch: 14 batch: 18 current batch loss: 0.07552163302898407\n",
            "epoch: 14 batch: 19 current batch loss: 0.07811085879802704\n",
            "epoch: 14 batch: 20 current batch loss: 0.09687414020299911\n",
            "epoch: 14 batch: 21 current batch loss: 0.09142933785915375\n",
            "epoch: 14 batch: 22 current batch loss: 0.08301406353712082\n",
            "epoch: 14 batch: 23 current batch loss: 0.07712879031896591\n",
            "epoch: 14 batch: 24 current batch loss: 0.07311715185642242\n",
            "epoch: 14 batch: 25 current batch loss: 0.07299581170082092\n",
            "epoch: 14 batch: 26 current batch loss: 0.08757840842008591\n",
            "epoch: 14 batch: 27 current batch loss: 0.07940070331096649\n",
            "epoch: 14 batch: 28 current batch loss: 0.06868129223585129\n",
            "epoch: 14 batch: 29 current batch loss: 0.10106789320707321\n",
            "epoch: 15 batch: 0 current batch loss: 0.06677328050136566\n",
            "epoch: 15 batch: 1 current batch loss: 0.08038242906332016\n",
            "epoch: 15 batch: 2 current batch loss: 0.07030525803565979\n",
            "epoch: 15 batch: 3 current batch loss: 0.08614800870418549\n",
            "epoch: 15 batch: 4 current batch loss: 0.070344939827919\n",
            "epoch: 15 batch: 5 current batch loss: 0.07235436886548996\n",
            "epoch: 15 batch: 6 current batch loss: 0.08395484834909439\n",
            "epoch: 15 batch: 7 current batch loss: 0.0894511267542839\n",
            "epoch: 15 batch: 8 current batch loss: 0.0771382674574852\n",
            "epoch: 15 batch: 9 current batch loss: 0.06972581893205643\n",
            "epoch: 15 batch: 10 current batch loss: 0.07188117504119873\n",
            "epoch: 15 batch: 11 current batch loss: 0.09784028679132462\n",
            "epoch: 15 batch: 12 current batch loss: 0.06309002637863159\n",
            "epoch: 15 batch: 13 current batch loss: 0.0691898837685585\n",
            "epoch: 15 batch: 14 current batch loss: 0.09444360435009003\n",
            "epoch: 15 batch: 15 current batch loss: 0.0676126480102539\n",
            "epoch: 15 batch: 16 current batch loss: 0.09272947907447815\n",
            "epoch: 15 batch: 17 current batch loss: 0.0860048159956932\n",
            "epoch: 15 batch: 18 current batch loss: 0.08491870015859604\n",
            "epoch: 15 batch: 19 current batch loss: 0.07740455865859985\n",
            "epoch: 15 batch: 20 current batch loss: 0.08155364543199539\n",
            "epoch: 15 batch: 21 current batch loss: 0.07539673894643784\n",
            "epoch: 15 batch: 22 current batch loss: 0.084381602704525\n",
            "epoch: 15 batch: 23 current batch loss: 0.06357195973396301\n",
            "epoch: 15 batch: 24 current batch loss: 0.06779219955205917\n",
            "epoch: 15 batch: 25 current batch loss: 0.08234361559152603\n",
            "epoch: 15 batch: 26 current batch loss: 0.07415032386779785\n",
            "epoch: 15 batch: 27 current batch loss: 0.0774536281824112\n",
            "epoch: 15 batch: 28 current batch loss: 0.07907790690660477\n",
            "epoch: 15 batch: 29 current batch loss: 0.04406040161848068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "----------------------"
      ],
      "metadata": {
        "id": "E2IB8sviqsir"
      },
      "id": "E2IB8sviqsir"
    },
    {
      "cell_type": "code",
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint.to(device))                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ],
      "metadata": {
        "id": "ubj7gcEjrs50",
        "outputId": "2a04643c-7b6f-4bd1-a397-806156bed084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ubj7gcEjrs50",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9778\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}